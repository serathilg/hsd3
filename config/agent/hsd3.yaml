# Defaults for HSD3 agent

name: 'hsd3'
device: ${device}
gamma: 0.99  # discount factor
polyak: 0.995  # polyak step size for updating target network
batch_size: 256
rpbuf_size: 1e6  # replay buffer size
rpbuf_device: auto
samples_per_update: 50  # perform updates every N samples
num_updates: ${agent.samples_per_update}
warmup_samples: 1000  # use first N samples to fill replay buffer
randexp_samples: 1000  # explore with random actions for first N samples
clip_grad_norm: 0.0  # only used if > 0
expectation_d: full  # or false, or a number to sample
action_interval: ???
dyne_updates: true
trace: false
norm_rewards: false

alpha_c: 1.0  # If optim_alpha is not null, this is just the initial alpha value
target_entropy_factor_c: 1.0 # If optim_alpha is not null
optim_alpha_c:  # Can be set to null to disable entropy coefficient learning and use target_entropy instead.
  _target_: torch.optim.Adam
  lr: 1e-4
alpha_d: 1.0
target_entropy_factor_d: 0.5 # of uniform policy
optim_alpha_d: ${agent.optim_alpha_c}

goal_space:
  robot: ???
  features: joints
  key: observation # observation item for accessing the goal space
  subsets: from_lo
  from_lo_eps: 0.1
  rank_min: 1
  rank_max: 99
  seed: ${seed}
  delta_actions: false
  mask_feats: null
lo:
  init_from: ???
