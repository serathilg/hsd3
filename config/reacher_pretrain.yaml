defaults:
  - common
  - agent: sacmt
  - _self_


hydra:
  run:
    dir: ./outputs/${hydra.job.name}.${robot}.${features}.${feature_dims}.${now:%Y-%m-%d-%H-%M-%S}/


horizon: 72
idle_steps: 0
robot: Reacher
features: joint_value
feature_dims: all
feature_rank: 1
feature_rank_max: 99
ctrl_eps: 0.7
precision: 0.1
max_new_tasks: 1000
task_weighting: uniform
downrank: 0.1
lp_new_task: 0.1
lp_eps: 0.1
eval_mode: running_avg
# eval_mode: reachability
# eval_mode: rollouts
max_steps: 5e7
combine_after_steps: 0
backprop_fallover: false
estimate_joint_spaces: null
init_model_from:
checkpoint_path: checkpoint-lo.pt

eval:
  episodes_per_task: 50
  interval: 100_000
  video:
    record_all: false
video: 
  interval: 500_000

distributed:
  num_actors: 1
  num_learners: 1
  rdvu_path: /tmp

slurm:
  gpus_per_task: 2
  cpus_per_task: ${env.train_procs}

visdom:
  offline: true

env:
  name: ContCtrlgsPreTraining-v1
  eval_procs: 20
  train_procs: 20
  args:
    robot: ${robot}
    features: ${features}
    idle_steps: ${idle_steps}
    max_steps: ${horizon}
    precision: ${precision}
    resample_features: soft
    goal_sampling: uniform
    hard_reset_interval: 100
    implicit_soft_resets: true
    ctrl_cost: 0.001
    fork: false # envs in single process to avoid overhead, still use many for network parallel efficiency

agent:
  gamma: auto_horizon
  samples_per_update: 1000
  rpbuf_size: 3e6
  num_updates: 100
  batch_size: 256
  randexp_samples: 10000
  warmup_samples: 10000
  flatten_obs: false
  per_task_alpha: false

model:
  pi: pi_d2rl_d_1024
  q: qd_d2rl_d_1024
  reachability: q_d2rl_d_1024

optim:
  pi:
    _target_: torch.optim.Adam
    lr: 1e-4
    fuse: true
  q: ${optim.pi}
  reachability: ${optim.q}
